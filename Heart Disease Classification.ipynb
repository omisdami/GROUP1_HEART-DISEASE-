{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Classification Project\n",
    "\n",
    "This notebook implements a comprehensive analysis and classification of heart disease data using multiple machine learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*force_all_finite.*\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The Heart Disease dataset dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.\n",
    "\n",
    "source: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
    "\n",
    "Here are the features:\n",
    "\n",
    "1. **age**: Age in years\n",
    "\n",
    "2. **sex**: Sex (1 = male, 0 = female)\n",
    "3. **cp**: Chest pain type\n",
    "   - Value 0: Typical angina\n",
    "   - Value 1: Atypical angina\n",
    "   - Value 2: Non-anginal pain\n",
    "   - Value 3: Asymptomatic\n",
    "4. **trestbps**: Resting blood pressure (in mm Hg)\n",
    "5. **chol**: Serum cholesterol in mg/dl\n",
    "6. **fbs**: Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "7. **restecg**: Resting electrocardiographic results\n",
    "   - Value 0: Normal\n",
    "   - Value 1: Having ST-T wave abnormality\n",
    "   - Value 2: Showing probable or definite left ventricular hypertrophy\n",
    "8. **thalach**: Maximum heart rate achieved\n",
    "9. **exang**: Exercise induced angina (1 = yes; 0 = no)\n",
    "10. **oldpeak**: ST depression induced by exercise relative to rest\n",
    "11. **slope**: The slope of the peak exercise ST segment\n",
    "   - Value 0: Downsloping\n",
    "   - Value 1: Flat (horizontal)\n",
    "   - Value 2: Upsloping\n",
    "12. **ca**: Number of major vessels colored by fluoroscopy (0-3)\n",
    "13. **thal**: Thalassemia\n",
    "   - Value 1: Normal\n",
    "   - Value 2: Fixed defect\n",
    "   - Value 3: Reversible defect\n",
    "14. **target**: Diagnosis of heart disease (0 = absence, 1 = presence)\n",
    "\n",
    "The names and social security numbers of the patients were recently removed from the database, replaced with dummy values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# igore FutureWarning where calling sort_value\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis and Data Preprocessing\n",
    "\n",
    "Let's examine and clean our dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initial Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# Find unique values for each column\n",
    "len_unique = {}\n",
    "for col in df.columns:\n",
    "    len_unique[col]=len(df[col].unique())\n",
    "print(\"\\nUnique values for each column:\", len_unique)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The initial data exploration reveals:**\n",
    "- Dataset shape: 1025 rows × 14 columns\n",
    "- No missing values in any columns\n",
    "- 723 duplicate rows identified\n",
    "- Features include both continuous measurements (age, blood pressure, cholesterol) and categorical variables (sex, chest pain type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "print(f\"Proportion of positive cases: {len(df[df['target'] == 1]) / len(df):.2%}\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.countplot(data=df, x='target')\n",
    "plt.title('Distribution of Heart Disease')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The target variable visualization shows:**\n",
    "- Binary classification problem (0 = no heart disease, 1 = heart disease present)\n",
    "- Nearly balanced dataset with approximately 51.3% positive cases\n",
    "- Distribution plot indicates good representation of both classes, which is beneficial for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Age and Gender Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Age distribution by target\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=df, x='target', y='age')\n",
    "plt.title('Age Distribution by Heart Disease')\n",
    "plt.xlabel('Heart Disease Present')\n",
    "plt.ylabel('Age')\n",
    "\n",
    "# Age distribution by target and gender\n",
    "sex_counts = df['sex'].value_counts(normalize=True) * 100\n",
    "print(\"Percentage of male: {:.2f}%\".format(sex_counts[1]), \n",
    "      \"| Percentage of female: {:.2f}%\".format(sex_counts[0]))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.boxplot(data=df, x='target', y='age', hue='sex', palette=['pink', 'lightblue'])\n",
    "plt.title('Age Distribution by Target and Gender')\n",
    "plt.xlabel('Heart Disease Present')\n",
    "plt.ylabel('Age')\n",
    "\n",
    "# Create custom legend handles\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='pink', label='Female'),\n",
    "                  Patch(facecolor='lightblue', label='Male')]\n",
    "\n",
    "# Add the custom legend\n",
    "plt.legend(handles=legend_elements, title='Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The age distribution plots reveal:**\n",
    "\n",
    "The dataset skews male (about 70% male and 30& female).\n",
    "\n",
    "**Patients with heart disease (target=1) do tend to be slightly younger on average** than those without. When we break it down by gender, we also see that:\n",
    "\n",
    "-   **Men who have heart disease** present at **younger ages** (their median age is lower) than **women who have heart disease.**\n",
    "-   Conversely, among those without heart disease, men again trend slightly younger than women, though the gap is smaller.\n",
    "\n",
    "This suggests a **possible earlier onset** (or earlier detection) of heart disease in men, whereas women who develop heart disease in this dataset are often older."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Clinical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical features distribution\n",
    "clinical_features = ['trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(clinical_features):\n",
    "    sns.boxplot(data=df, x='target', y=feature, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of key clinical measurements:**\n",
    "-   **Resting Blood Pressure (`trestbps`)**: The median resting blood pressure is **almost the same as patients without disease**, though there are outliers with very high BP in both groups. Overall, resting BP does not appear to differ strongly by disease status in this sample.\n",
    "\n",
    "-   **Cholesterol (`chol`)**: CBoth groups show substantial overlap (and some extreme outliers ≥ 500 mg/dl), with only a small difference in the median levels. This suggests cholesterol alone may not be a highly discriminative feature in this particular dataset.\n",
    "\n",
    "-   **Maximum Heart Rate (`thalach`)**: The “Disease” group shows a higher median and overall distribution for maximum achieved heart rate compared to the no‐disease group. While it may seem counterintuitive (as one might expect limited heart‐rate response in more diseased hearts), in this dataset higher max‐HR measurements appear associated with having heart disease—likely reflecting the presence of individuals who can mount a stress response but still manifest other disease indicators.\n",
    "\n",
    "-   **ST Depression (`oldpeak`)**: Those **without disease** have a **higher median** ST depression, whereas patients with disease often show lower ST depression---though the \"no disease\" group has considerable variability and a number of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Medical Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key risk factors\n",
    "risk_factors = ['cp','fbs', 'exang', 'slope', 'ca', 'thal']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, factor in enumerate(risk_factors):\n",
    "    sns.countplot(data=df, x=factor, hue='target', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{factor} Distribution by Target')\n",
    "    axes[idx].legend(['No Disease', 'Disease'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical risk factors show distinct patterns:**\n",
    "- **Chest Pain Type (`cp`)**: **Typical angina (CP=0)** appears to be most prevalent among the no‐disease group, while atypical angina (CP=1), non‐anginal pain (CP=2), and asymptomatic presentations (CP=3) tend to be more common in the group that does have heart disease with **non-anginal pain** being most common in those with disease.\n",
    "\n",
    "- **Fasting Blood Sugar (`fbs`)**: The vast majority of both groups have a **normal fasting blood sugar (fbs=0).** A smaller share of each group shows elevated blood sugar (fbs=1).\n",
    "\n",
    "-   **Exercise-Induced Angina (`exang`)**: Having *no* exercise-induced angina (exang=0) is **more frequent** among those **with heart disease**; by contrast, exang=1 appears more in the no‐disease group.\n",
    "\n",
    "-   **Slope of ST Segment (`slope`)**: A **downsloping ST segment (slope=0)** is more associated with those who **do not have heart disease**, whereas an **upsloping ST segment (slope=2)** is much more common among disease‐positive individuals. **Flat ST slope (slope=2)** is present much more often in the no disease group vs those with disease but has a higher proportion of those with disease compared to slope 0.\n",
    "\n",
    "-   **Number of Major Vessels (`ca`)**: A large fraction of those **with disease** have **0 major vessels** colored by fluoroscopy, while those with **1 or more** vessels colored tend to be more in the no‐disease group.\n",
    "\n",
    "-   **Thalassemia Status (`thal`)**: \"Reversible defect\" (thal=3) occurs more often in the no disease group, along with \"normal\" (thal=1). \"Fixed defect\" (thal=2) occurs often in people with disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature correlations with target\n",
    "correlations = df.corr()['target'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlations.plot(kind='bar')\n",
    "plt.title('Feature Correlations with Target')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation Analysis:**\n",
    "\n",
    "#### **Key Correlations with the target (Heart Disease):**\n",
    "1. **Chest Pain Type (cp)** → *0.43*\n",
    "   - Moderate positive correlation\n",
    "   - Certain chest pain types indicate higher likelihood of heart disease\n",
    "   - Significant diagnostic indicator\n",
    "\n",
    "2. **Maximum Heart Rate (thalach)** → *0.42*\n",
    "   - Moderate positive correlation\n",
    "   - Higher exercise heart rates linked to increased disease probability\n",
    "\n",
    "3. **ST Segment Slope (slope)** → *0.35*\n",
    "   - Moderate positive correlation\n",
    "   - Steeper slopes during peak exercise associated with heart disease\n",
    "\n",
    "4. **Exercise-Induced Angina (exang)** → *-0.44*\n",
    "   - Moderate negative correlation\n",
    "   - Counterintuitive finding: exercise-induced chest pain linked to lower disease probability\n",
    "   - May indicate data imbalance or specific dataset patterns\n",
    "\n",
    "5. **ST Depression (oldpeak)** → *-0.44*\n",
    "   - Moderate negative correlation\n",
    "   - Unexpected finding: greater ST depression linked to lower disease risk\n",
    "   - Requires further clinical investigation\n",
    "\n",
    "6. **Number of Major Vessels (ca)** → *-0.38*\n",
    "   - Moderate negative correlation\n",
    "   - Unusual finding: more blocked vessels associated with lower risk\n",
    "   - Possible feature misinterpretation\n",
    "\n",
    "7. **Thalassemia (thal)** → *-0.34*\n",
    "   - Moderate negative correlation\n",
    "\n",
    "8. **Sex** → *-0.28*\n",
    "   - Males show slightly lower disease probability\n",
    "\n",
    "9. **Age** → *-0.23*\n",
    "   - Unexpected negative correlation with age\n",
    "   - Warrants further investigation\n",
    "\n",
    "#### **Notable Feature-to-Feature Correlations:**\n",
    "\n",
    "1. **oldpeak → slope**: *-0.58*\n",
    "  - Strong negative correlation between ST depression and slope\n",
    "\n",
    "2. **cp → exang**: *-0.40*\n",
    "  - Inverse relationship between chest pain types and exercise-induced angina\n",
    "\n",
    "3. **exang → thalach**: *-0.38*\n",
    "  - Exercise-induced angina associated with lower max heart rates\n",
    "\n",
    "4. **cp → thalach**: *0.31*\n",
    "  - Certain chest pain types correlate with higher exercise heart rates\n",
    "\n",
    "#### **Summary of Strongest Correlations**\n",
    "\n",
    "##### Positive Correlations with Heart Disease\n",
    "- Chest pain type (cp)\n",
    "- Maximum heart rate (thalach)\n",
    "- ST Segment Slope (slope)\n",
    "\n",
    "##### Negative Correlations with Heart Disease\n",
    "- ST depression (oldpeak)\n",
    "- Exercise induced angina (exang)\n",
    "- Number of vessels (ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "\n",
    "# Function to remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "    return df_clean\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "categorical_features = ['cp', 'thal', 'slope', 'restecg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and feature engineering\n",
    "\n",
    "# Handle outliers in numerical features\n",
    "df_clean = remove_outliers(df, numerical_features)\n",
    "print(f\"Rows after outlier removal: {len(df_clean)} (removed {len(df) - len(df_clean)} rows)\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop('target', axis=1)\n",
    "y = df_clean['target']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split and scaling\n",
    "\n",
    "# Split the data\n",
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale only the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = X_train_unscaled.copy()\n",
    "X_test = X_test_unscaled.copy()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train_unscaled[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test_unscaled[numerical_features])\n",
    "\n",
    "print(\"\\nTraining set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "print(\"\\nFeatures after one-hot encoding:\", X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Feature_selection\n",
    "importlib.reload(Feature_selection)\n",
    "from Feature_selection import autoFeatureSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `num_output_features` specifies the number of features to select for each method.\n",
    "# The `autoFeatureSelector` function applies multiple feature selection methods \n",
    "# (e.g., Pearson correlation, chi-square test, RFE, etc.) and combines their results\n",
    "# by voting to identify the most important features.\n",
    "\n",
    "best_features = autoFeatureSelector(\n",
    "    X,\n",
    "    y,\n",
    "    methods=['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm'],  # List of feature selection methods\n",
    "    num_output_features=5  # Number of features to select per method\n",
    ")\n",
    "\n",
    "# Output the final list of selected features after voting\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, identify which encoded columns correspond to each categorical feature                                                 \n",
    "encoded_feature_map = {                                                                                                        \n",
    "    'cp': [col for col in X_train.columns if col.startswith('cp_')],                                                           \n",
    "    'thal': [col for col in X_train.columns if col.startswith('thal_')]                                                        \n",
    "}   \n",
    "\n",
    "# Create list of columns to keep                                                                                               \n",
    "columns_to_keep = []                                                                                                           \n",
    "for feature in best_features:                                                                                                  \n",
    "    if feature in ['cp', 'thal']:                                                                                              \n",
    "        # If feature is categorical, add all its encoded columns                                                               \n",
    "        columns_to_keep.extend(encoded_feature_map[feature])                                                                   \n",
    "    else:                                                                                                                      \n",
    "        # If feature is not categorical, add it directly                                                                       \n",
    "        columns_to_keep.append(feature) \n",
    "\n",
    "# Select features for both train and test sets                                                                                 \n",
    "X_train_fs = X_train[columns_to_keep]                                                                                          \n",
    "X_test_fs = X_test[columns_to_keep]  \n",
    "\n",
    "print(\"\\nTraining set shape:\", X_train_fs.shape)\n",
    "print(\"Testing set shape:\", X_test_fs.shape)\n",
    "\n",
    "X_train_fs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression_withOut_featureSelection\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Logistic Regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Regression Model accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Define parameter grid for logistic regression\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],\n",
    "    'class_weight': ['balanced', {0:1, 1:2}, {0:1, 1:3}]\n",
    "}\n",
    "\n",
    "# Grid search with cross validation\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    param_grid,\n",
    "    scoring='recall',\n",
    "    cv=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and model\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
    "y_pred_tuned = grid_search.predict(X_test)\n",
    "\n",
    "# Print results for tuned model\n",
    "print(\"\\nTuned Logistic Regression Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# Feature selected model\n",
    "logistic_fs = LogisticRegression(max_iter=1000)\n",
    "logistic_fs.fit(X_train_fs, y_train)\n",
    "y_pred_fs = logistic_fs.predict(X_test_fs)\n",
    "\n",
    "# Print results for feature selected model\n",
    "print(\"\\nFeature Selected Logistic Regression Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_fs))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_fs))\n",
    "\n",
    "# Grid search with feature selection\n",
    "grid_search_fs = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    param_grid,\n",
    "    scoring='recall',\n",
    "    cv=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search on feature selected data\n",
    "grid_search_fs.fit(X_train_fs, y_train)\n",
    "\n",
    "# Get predictions\n",
    "print(\"\\nBest Parameters (Feature Selected):\", grid_search_fs.best_params_)\n",
    "y_pred_fs_tuned = grid_search_fs.predict(X_test_fs)\n",
    "\n",
    "# Print results for tuned feature selected model\n",
    "print(\"\\nTuned Feature Selected Logistic Regression Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_fs_tuned))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_fs_tuned))\n",
    "\n",
    "# Plot confusion matrices for all four models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Define model names and predictions\n",
    "models = {\n",
    "    'Baseline': y_pred,\n",
    "    'Tuned': y_pred_tuned,\n",
    "    'Feature Selected': y_pred_fs,\n",
    "    'Tuned Feature Selected': y_pred_fs_tuned\n",
    "}\n",
    "\n",
    "for (title, predictions), ax in zip(models.items(), axes.ravel()):\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'Logistic Regression\\n{title}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create comparison DataFrame\n",
    "metrics = {\n",
    "    'Model': ['Baseline', 'Tuned', 'Feature Selected', 'Tuned Feature Selected'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred),\n",
    "        accuracy_score(y_test, y_pred_tuned),\n",
    "        accuracy_score(y_test, y_pred_fs),\n",
    "        accuracy_score(y_test, y_pred_fs_tuned)\n",
    "    ]\n",
    "}\n",
    "\n",
    "for i, predictions in enumerate([y_pred, y_pred_tuned, y_pred_fs, y_pred_fs_tuned]):\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    metrics[f'Precision (Class 1)'] = metrics.get(f'Precision (Class 1)', []) + [report['1']['precision']]\n",
    "    metrics[f'Recall (Class 1)'] = metrics.get(f'Recall (Class 1)', []) + [report['1']['recall']]\n",
    "    metrics[f'F1-score (Class 1)'] = metrics.get(f'F1-score (Class 1)', []) + [report['1']['f1-score']]\n",
    "\n",
    "results_df = pd.DataFrame(metrics)\n",
    "results_df = results_df.set_index('Model')\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(\"------------------------\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression_with_featureSelection\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "logistic_model_fs = LogisticRegression(max_iter=1000)\n",
    "logistic_model_fs.fit(X_train_fs, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_fs = logistic_model_fs.predict(X_test_fs)\n",
    "accuracy = accuracy_score(y_test, y_pred_fs)\n",
    "\n",
    "print(\"Logistic Regression Model with feature selection accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Default SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "sgd_default = SGDClassifier(random_state=42)\n",
    "\n",
    "# Train model\n",
    "sgd_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_default = sgd_default.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model performance\n",
    "print(\"Default SGD Classifier Results:\")\n",
    "print(\"---------------------------\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_default))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_default), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Default SGD Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation analysis\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(sgd_default, X_train, y_train, cv=5)\n",
    "print(\"\\nCross-validation scores:\", cv_scores)\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    sgd_default, X_train, y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, n_jobs=-1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')\n",
    "plt.plot(train_sizes, val_scores.mean(axis=1), label='Cross-validation score')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves for SGD Classifier')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "-   For small training sets, the **training accuracy** is high (e.g., ~0.92), indicating potential overfitting.\n",
    "-   The **cross-validation score** initially lags behind, then fluctuates around 0.80--0.82 as more data is added.\n",
    "-   As training size increases, the training score drops and the CV score settles, suggesting the model generalizes better but still exhibits variability.\n",
    "-   Overall, the two curves do not fully converge but come closer around mid--high 0.8s, hinting that adding even more data (or tuning hyperparameters) may improve or stabilize the model performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'loss': ['modified_huber', 'log_loss'],          # Focus on probability-calibrated losses\n",
    "    'penalty': ['l2', 'elasticnet'],                # Regularization methods\n",
    "    'alpha': [0.0001, 0.001],                       # Lower regularization for better recall\n",
    "    'learning_rate': ['adaptive'],                  # Adaptive learning rate\n",
    "    'eta0': [0.01, 0.1],                           # Initial learning rate\n",
    "    'class_weight': [{0:1, 1:2}, {0:1, 1:3}]       # Try different false negative penalties\n",
    "}\n",
    "\n",
    "# Perform grid search optimizing for recall\n",
    "grid_search = GridSearchCV(SGDClassifier(random_state=42, \n",
    "                                       max_iter=1000, \n",
    "                                       tol=1e-3,\n",
    "                                       early_stopping=True,\n",
    "                                       validation_fraction=0.1),\n",
    "                           param_grid,\n",
    "                           scoring='recall',  # Optimize for minimizing false negatives\n",
    "                           cv=5,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "# Compare baseline and tuned model metrics\n",
    "print(\"\\nBaseline SGD Model Metrics:\")\n",
    "print(\"---------------------------\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_default))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "\n",
    "print(\"\\nTuned SGD Model Metrics:\")\n",
    "print(\"------------------------\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Baseline model confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_default), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1)\n",
    "ax1.set_title(\"Baseline SGD Model\\nConfusion Matrix\")\n",
    "ax1.set_xlabel(\"Predicted\")\n",
    "ax1.set_ylabel(\"Actual\")\n",
    "\n",
    "# Tuned model confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_tuned),\n",
    "            annot=True, fmt=\"d\", cmap=\"Greens\", ax=ax2)\n",
    "ax2.set_title(\"Tuned SGD Model\\nConfusion Matrix\") \n",
    "ax2.set_xlabel(\"Predicted\")\n",
    "ax2.set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Feature Selection and Re-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train basic SGD model with selected features\n",
    "sgd_fs = SGDClassifier(random_state=42)\n",
    "sgd_fs.fit(X_train_fs, y_train)\n",
    "\n",
    "# Train grid search tuned model with selected features\n",
    "sgd_fs_tuned = GridSearchCV(SGDClassifier(random_state=42, \n",
    "                                       max_iter=1000, \n",
    "                                       tol=1e-3,\n",
    "                                       early_stopping=True,\n",
    "                                       validation_fraction=0.1),\n",
    "                           param_grid,\n",
    "                           scoring='recall',  # Optimize for minimizing false negatives\n",
    "                           cv=5,\n",
    "                           verbose=1)\n",
    "sgd_fs_tuned.fit(X_train_fs, y_train)\n",
    "\n",
    "# Make predictions for all models\n",
    "y_pred_train_sgd_fs = sgd_fs.predict(X_train_fs)\n",
    "y_pred_test_sgd_fs = sgd_fs.predict(X_test_fs)\n",
    "y_pred_test_fs_tuned = sgd_fs_tuned.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies\n",
    "accuracy_train_sgd_fs = accuracy_score(y_train, y_pred_train_sgd_fs)\n",
    "accuracy_test_sgd_fs = accuracy_score(y_test, y_pred_test_sgd_fs)\n",
    "accuracy_test_tuned = accuracy_score(y_test, y_pred_test_fs_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports for comparison\n",
    "print(\"\\nFeature Selected SGD Model Metrics:\")\n",
    "print(\"---------------------------\")\n",
    "print(\"\\nAccuracy:\", accuracy_train_sgd_fs)\n",
    "print(\"\\nAccuracy:\", accuracy_test_sgd_fs)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_sgd_fs))\n",
    "\n",
    "print(\"\\nTuned Feature Selected SGD Model Metrics:\")\n",
    "print(\"---------------------------\")\n",
    "print(\"\\nAccuracy:\", accuracy_test_tuned)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_fs_tuned))\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Baseline model confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test_sgd_fs), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1)\n",
    "ax1.set_title(\"Feature Selected SGD Model\\nConfusion Matrix\")\n",
    "ax1.set_xlabel(\"Predicted\")\n",
    "ax1.set_ylabel(\"Actual\")\n",
    "\n",
    "# Tuned model confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test_fs_tuned),\n",
    "            annot=True, fmt=\"d\", cmap=\"Greens\", ax=ax2)\n",
    "ax2.set_title(\"Tuned Feature Selected SGD Model\\nConfusion Matrix\") \n",
    "ax2.set_xlabel(\"Predicted\")\n",
    "ax2.set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the metrics                                                                        \n",
    "metrics = {                                                                                                       \n",
    "    'Model': ['Baseline SGD', 'Tuned SGD', 'Feature Selected SGD', 'Tuned Feature Selected SGD'],                 \n",
    "    'Accuracy': [                                                                                                 \n",
    "        accuracy_score(y_test, y_pred_default),                                                                   \n",
    "        accuracy_score(y_test, y_pred_tuned),                                                                     \n",
    "        accuracy_test_sgd_fs,                                                                                     \n",
    "        accuracy_test_tuned                                                                                    \n",
    "    ]                                                                                                             \n",
    "}                                                                                                                 \n",
    "                                                                                                                \n",
    "# Create detailed metrics for each model                                                                          \n",
    "for i, (y_true, y_pred) in enumerate([                                                                            \n",
    "    (y_test, y_pred_default),                                                                                     \n",
    "    (y_test, y_pred_tuned),                                                                                       \n",
    "    (y_test, y_pred_test_sgd_fs),                                                                              \n",
    "    (y_test, y_pred_test_fs_tuned)                                                                             \n",
    "]):                                                                                                               \n",
    "    report = classification_report(y_true, y_pred, output_dict=True)                                              \n",
    "    metrics[f'Precision (Class 1)'] = metrics.get(f'Precision (Class 1)', []) + [report['1']['precision']]        \n",
    "    metrics[f'Recall (Class 1)'] = metrics.get(f'Recall (Class 1)', []) + [report['1']['recall']]                 \n",
    "    metrics[f'F1-score (Class 1)'] = metrics.get(f'F1-score (Class 1)', []) + [report['1']['f1-score']]           \n",
    "                                                                                                                \n",
    "# Create DataFrame and display with formatting                                                                    \n",
    "results_df = pd.DataFrame(metrics)                                                                                \n",
    "results_df = results_df.set_index('Model')                                                                        \n",
    "print(\"\\nModel Comparison Results:\")                                                                              \n",
    "print(\"------------------------\")                                                                                 \n",
    "print(results_df.round(3))     \n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Baseline model confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_default), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1)\n",
    "ax1.set_title(\"Baseline SGD Model\\nConfusion Matrix\")\n",
    "ax1.set_xlabel(\"Predicted\")\n",
    "ax1.set_ylabel(\"Actual\")\n",
    "\n",
    "# Tuned model confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_tuned),\n",
    "            annot=True, fmt=\"d\", cmap=\"Greens\", ax=ax2)\n",
    "ax2.set_title(\"Tuned SGD Model\\nConfusion Matrix\") \n",
    "ax2.set_xlabel(\"Predicted\")\n",
    "ax2.set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Baseline model confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test_sgd_fs), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1)\n",
    "ax1.set_title(\"Feature Selected SGD Model\\nConfusion Matrix\")\n",
    "ax1.set_xlabel(\"Predicted\")\n",
    "ax1.set_ylabel(\"Actual\")\n",
    "\n",
    "# Tuned model confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test_fs_tuned),\n",
    "            annot=True, fmt=\"d\", cmap=\"Greens\", ax=ax2)\n",
    "ax2.set_title(\"Tuned Feature Selected SGD Model\\nConfusion Matrix\") \n",
    "ax2.set_xlabel(\"Predicted\")\n",
    "ax2.set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis                                                                                                                 \n",
    "                                                                                                                            \n",
    "The comparison of different SGD model variants reveals several interesting insights:                                           \n",
    "                                                                                                                            \n",
    "1. **Baseline SGD Model**                                                                                                      \n",
    "- Achieved accuracy of 84.5%                                                                                                   \n",
    "- Balanced precision (0.848) and recall (0.842)                                                                                \n",
    "- F1-score of 0.845                                                                                                            \n",
    "- Shows good overall performance with balanced predictions                                                                     \n",
    "                                                                                                                            \n",
    "2. **Tuned SGD Model**                                                                                                         \n",
    "- Slightly lower accuracy at 82.4%                                                                                             \n",
    "- Lower precision (0.765) but higher recall (0.938)                                                                            \n",
    "- F1-score of 0.843, nearly matching baseline                                                                                  \n",
    "- Hyperparameter tuning traded some precision for better recall                                                                \n",
    "- Better at identifying positive cases (fewer false negatives)                                                                 \n",
    "                                                                                                                            \n",
    "3. **Feature Selected SGD Model**                                                                                              \n",
    "- Accuracy of 69.0%                                                                                                            \n",
    "- Good precision (0.841) but very low recall (0.473)                                                                                    \n",
    "- F1-score of 0.605                                                                                                            \n",
    "- Feature selection resulted in a significant drop in accuracy                                                                \n",
    "- Much worse recall                                                                \n",
    "                                                                                                                            \n",
    "4. **Tuned Feature Selected SGD Model**                                                                                        \n",
    "- Accuracy of 82.8%                                                                                                            \n",
    "- Lower precision (0.776) but higher recall (0.925)                                                                            \n",
    "- F1-score of 0.844                                                                                                           \n",
    "- Similar pattern to non-feature-selected tuned model                                                                          \n",
    "- Prioritizes recall over precision                                                                                            \n",
    "                                                                                                                            \n",
    "**Key Takeaways:**                                                                                                             \n",
    "- The baseline model performed surprisingly well, suggesting the problem may not require complex tuning                        \n",
    "- Hyperparameter tuning consistently improved recall at the cost of precision (as it was optimized for recall)                                                 \n",
    "- Feature selection alone showed a significant drop in performance                                                          \n",
    "- If false negatives are more costly than false positives, the tuned models would be preferred despite lower overall accuracy  \n",
    "- The best model choice depends on whether balanced performance (baseline) or higher recall (tuned) is more                    \n",
    "important for the application                                                                                                  \n",
    "                                                                                                                            \n",
    "The confusion matrices visually confirm these patterns, showing how the different variants trade off between false             \n",
    "positives and false negatives.                                                                                                 \n",
    "                                                                                                                            \n",
    "In the case of heart disease, false negatives can be very dangerous and it would be preferred to minimize them. Hence, the best \n",
    "model for this case seems to be the tuned SGD model with all the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model (No Feature Selection, No Hyperparameter Tuning)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_base = RandomForestClassifier()\n",
    "rf_model_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_base = rf_model_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_base = accuracy_score(y_test, y_pred_base)\n",
    "print(f'Accuracy: {accuracy_base}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline with Hyperparamaters Tuning (No Feature Selection, with GridSearchCV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  \n",
    "    'max_depth': [2, 3, 5],  \n",
    "    'min_samples_split': [2, 10, 20],  \n",
    "    'min_samples_leaf': [1, 5, 10],  \n",
    "    'max_features': ['sqrt', 'log2']  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_base_tuned = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid,\n",
    "    cv = 5, \n",
    "    scoring = 'recall',\n",
    "    n_jobs = -1,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "rf_model_base_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_base_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_base_tuned = rf_model_base_tuned.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_base_tuned = accuracy_score(y_test, y_pred_base_tuned)\n",
    "print(f'Accuracy: {accuracy_base_tuned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature-Selected Model (With Feature Selection, No GridSearchCV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_fs = RandomForestClassifier()\n",
    "rf_model_fs.fit(X_train_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fs = rf_model_fs.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_fs = accuracy_score(y_test, y_pred_fs)\n",
    "print(f'Accuracy: {accuracy_fs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature-Selected with Hyperparameter Tuning (With Feature Selection, With GridSearchCV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_fs_tuned = GridSearchCV(\n",
    "    RandomForestClassifier(), \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs = -1, \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "rf_model_fs_tuned.fit(X_train_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_fs_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fs_tuned = rf_model_fs_tuned.best_estimator_.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_fs_tuned = accuracy_score(y_test, y_pred_fs_tuned)\n",
    "print(f'Accuracy: {accuracy_fs_tuned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for 4 different models\n",
    "y_pred_1 = rf_model_base.predict(X_test)         # Model 1: Clean data, no feature selection\n",
    "y_pred_2 = rf_model_base_tuned.best_estimator_.predict(X_test)   # Model 2: Clean data, no feature selection, with GridSearchCV\n",
    "y_pred_3 = rf_model_fs.predict(X_test_fs)           # Model 3: Clean data, with feature selection\n",
    "y_pred_4 = rf_model_fs_tuned.best_estimator_.predict(X_test_fs)    # Model 4: Clean data, feature selection, with GridSearchCV\n",
    "\n",
    "# Compute confusion matrices\n",
    "cm_1 = confusion_matrix(y_test, y_pred_base)\n",
    "cm_2 = confusion_matrix(y_test, y_pred_base_tuned)\n",
    "cm_3 = confusion_matrix(y_test, y_pred_fs)\n",
    "cm_4 = confusion_matrix(y_test, y_pred_fs_tuned)\n",
    "\n",
    "# Create subplots (2 rows, 2 columns)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Define model names for better visualization\n",
    "titles = [\n",
    "    \"Model 1: Baseline (No FS, No GridSearch)\",\n",
    "    \"Model 2: Baseline (No FS, With GridSearch)\",\n",
    "    \"Model 3: With Feature Selection (No GridSearch)\",\n",
    "    \"Model 4: With Feature Selection (With GridSearch)\"\n",
    "]\n",
    "\n",
    "# Function to plot confusion matrix using seaborn heatmap\n",
    "def plot_confusion_matrix(ax, cm, title):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=[\"No Disease\", \"Disease\"], yticklabels=[\"No Disease\", \"Disease\"], ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "\n",
    "# Plot all four confusion matrices\n",
    "for ax, cm, title in zip(axes.ravel(), [cm_1, cm_2, cm_3, cm_4], titles):\n",
    "    plot_confusion_matrix(ax, cm, title)\n",
    "\n",
    "# Adjust layout for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion on Model Performance and Overfitting**\n",
    "\n",
    "1. **Overfitting in the Baseline Models**\n",
    "   - **Before Feature Selection (Model 1 & Model 2)**\n",
    "     - Model 1 (Baseline, No FS, No GridSearch) achieves perfect classification:\n",
    "       - True Positives (TP): 146\n",
    "       - True Negatives (TN): 144\n",
    "       - No misclassifications (0 FP, 0 FN).\n",
    "       - This suggests the model might be overfitting, as perfect accuracy on a real-world dataset is uncommon.\n",
    "     - Model 2 (Baseline, No FS, With GridSearch) introduces misclassifications:\n",
    "       - FP increases to 16, FN increases to 9.\n",
    "       - A slight drop in accuracy, indicating that GridSearchCV might be tuning hyperparameters in a way that reduces overfitting.\n",
    "\n",
    "2. **Feature Selection Impacts Generalization**\n",
    "   - **After Feature Selection (Model 3 & Model 4)**\n",
    "     - Model 3 (With Feature Selection, No GridSearch):\n",
    "       - FP: 0, FN: 4.\n",
    "       - Slightly more misclassifications compared to Model 1.\n",
    "       - While accuracy decreases slightly, this indicates better generalization, as the model is no longer memorizing the dataset.\n",
    "     - Model 4 (With Feature Selection, With GridSearch):\n",
    "       - FP increases to 19, FN increases to 17.\n",
    "       - This suggests that hyperparameter tuning, specifically adjusting **max_depth**, helps control overfitting and improves the model's ability to perform well on unseen data.\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **Baseline Model 1 exhibits possible overfitting** (perfect accuracy).\n",
    "- **Feature selection improves generalization but introduces more misclassifications** (Models 3 & 4).\n",
    "- **GridSearchCV helps prevent overfitting** by tuning parameters like **max_depth**, leading to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_base = SVC(random_state=42)\n",
    "svc_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_svc_base = svc_base.predict(X_train)\n",
    "y_pred_svc_base = svc_base.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_svc_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy_svc_base = accuracy_score(y_train, y_pred_train_svc_base)\n",
    "test_accuracy_svc_base = accuracy_score(y_test, y_pred_svc_base)\n",
    "print(\"SVC baseline training dataset accuracy score\", training_accuracy_svc_base)\n",
    "print(\"SVC baseline testing dataset accuracy score\", test_accuracy_svc_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Base Model Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000], \n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "    'kernel': ['rbf']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid = GridSearchCV(SVC(), svc_param_grid, refit=True, verbose=1)  # refits the estimator using the best found parameters\n",
    "svc_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid_predictions = svc_grid.predict(X_test)\n",
    "confusion_matrix(y_test, svc_grid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, svc_grid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Feature-Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_fs = SVC(random_state=42)\n",
    "svc_fs.fit(X_train_fs, y_train)\n",
    "y_predict_train_svc_fs = svc_fs.predict(X_train_fs)\n",
    "y_predict_test_svc_fs = svc_fs.predict(X_test_fs)\n",
    "accuracy_train_svc_fs = accuracy_score(y_train, y_predict_train_svc_fs)\n",
    "accuracy_test_svc_fs = accuracy_score(y_test, y_predict_test_svc_fs)\n",
    "print(\"SVC feature selected train dataset accuracy score:\", accuracy_train_svc_fs)\n",
    "print(\"SVC feature selected test dataset accuracy score:\", accuracy_test_svc_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Feature-Selected Hyperparameter tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_fs_grid = GridSearchCV(SVC(), svc_param_grid, refit=True, verbose=1)  # refits the estimator using the best found parameters\n",
    "svc_fs_grid.fit(X_train_fs, y_train)\n",
    "y_predict_train_svc_fs_grid = svc_fs_grid.predict(X_train_fs)\n",
    "y_predict_test_svc_fs_grid = svc_fs_grid.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_svc_fs_grid = accuracy_score(y_train, y_predict_train_svc_fs_grid)\n",
    "accuracy_test_svc_fs_grid = accuracy_score(y_test, y_predict_test_svc_fs_grid)\n",
    "print(\"SVC feature selected TUNED train dataset accuracy score:\", accuracy_train_svc_fs_grid)\n",
    "print(\"SVC feature selected TUNED test dataset accuracy score:\", accuracy_test_svc_fs_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_predict_test_svc_fs_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
