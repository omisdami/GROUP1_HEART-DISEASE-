{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Classification Project\n",
    "\n",
    "This notebook implements a comprehensive analysis and classification of heart disease data using multiple machine learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*force_all_finite.*\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The Heart Disease dataset dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.\n",
    "\n",
    "source: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
    "\n",
    "Here are the features:\n",
    "\n",
    "1. **age**: Age in years\n",
    "\n",
    "2. **sex**: Sex (1 = male, 0 = female)\n",
    "3. **cp**: Chest pain type\n",
    "   - Value 0: Typical angina\n",
    "   - Value 1: Atypical angina\n",
    "   - Value 2: Non-anginal pain\n",
    "   - Value 3: Asymptomatic\n",
    "4. **trestbps**: Resting blood pressure (in mm Hg)\n",
    "5. **chol**: Serum cholesterol in mg/dl\n",
    "6. **fbs**: Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "7. **restecg**: Resting electrocardiographic results\n",
    "   - Value 0: Normal\n",
    "   - Value 1: Having ST-T wave abnormality\n",
    "   - Value 2: Showing probable or definite left ventricular hypertrophy\n",
    "8. **thalach**: Maximum heart rate achieved\n",
    "9. **exang**: Exercise induced angina (1 = yes; 0 = no)\n",
    "10. **oldpeak**: ST depression induced by exercise relative to rest\n",
    "11. **slope**: The slope of the peak exercise ST segment\n",
    "   - Value 0: Downsloping\n",
    "   - Value 1: Flat (horizontal)\n",
    "   - Value 2: Upsloping\n",
    "12. **ca**: Number of major vessels colored by fluoroscopy (0-3)\n",
    "13. **thal**: Thalassemia\n",
    "   - Value 1: Normal\n",
    "   - Value 2: Fixed defect\n",
    "   - Value 3: Reversible defect\n",
    "14. **target**: Diagnosis of heart disease (0 = absence, 1 = presence)\n",
    "\n",
    "The names and social security numbers of the patients were recently removed from the database, replaced with dummy values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# igore FutureWarning where calling sort_value\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis and Data Preprocessing\n",
    "\n",
    "Let's examine and clean our dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initial Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# Find unique values for each column\n",
    "len_unique = {}\n",
    "for col in df.columns:\n",
    "    len_unique[col]=len(df[col].unique())\n",
    "print(\"\\nUnique values for each column:\", len_unique)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The initial data exploration reveals:**\n",
    "- Dataset shape: 1025 rows × 14 columns\n",
    "- No missing values in any columns\n",
    "- 723 duplicate rows identified\n",
    "- Features include both continuous measurements (age, blood pressure, cholesterol) and categorical variables (sex, chest pain type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "print(f\"Proportion of positive cases: {len(df[df['target'] == 1]) / len(df):.2%}\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.countplot(data=df, x='target')\n",
    "plt.title('Distribution of Heart Disease')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The target variable visualization shows:**\n",
    "- Binary classification problem (0 = no heart disease, 1 = heart disease present)\n",
    "- Nearly balanced dataset with approximately 51.3% positive cases\n",
    "- Distribution plot indicates good representation of both classes, which is beneficial for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Age and Gender Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Age distribution by target\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=df, x='target', y='age')\n",
    "plt.title('Age Distribution by Heart Disease')\n",
    "plt.xlabel('Heart Disease Present')\n",
    "plt.ylabel('Age')\n",
    "\n",
    "# Age distribution by target and gender\n",
    "sex_counts = df['sex'].value_counts(normalize=True) * 100\n",
    "print(\"Percentage of male: {:.2f}%\".format(sex_counts[1]), \n",
    "      \"| Percentage of female: {:.2f}%\".format(sex_counts[0]))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.boxplot(data=df, x='target', y='age', hue='sex', palette=['pink', 'lightblue'])\n",
    "plt.title('Age Distribution by Target and Gender')\n",
    "plt.xlabel('Heart Disease Present')\n",
    "plt.ylabel('Age')\n",
    "\n",
    "# Create custom legend handles\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='pink', label='Female'),\n",
    "                  Patch(facecolor='lightblue', label='Male')]\n",
    "\n",
    "# Add the custom legend\n",
    "plt.legend(handles=legend_elements, title='Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The age distribution plots reveal:**\n",
    "\n",
    "The dataset skews male (about 70% male and 30& female).\n",
    "\n",
    "**Patients with heart disease (target=1) do tend to be slightly younger on average** than those without. When we break it down by gender, we also see that:\n",
    "\n",
    "-   **Men who have heart disease** present at **younger ages** (their median age is lower) than **women who have heart disease.**\n",
    "-   Conversely, among those without heart disease, men again trend slightly younger than women, though the gap is smaller.\n",
    "\n",
    "This suggests a **possible earlier onset** (or earlier detection) of heart disease in men, whereas women who develop heart disease in this dataset are often older."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Clinical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical features distribution\n",
    "clinical_features = ['trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(clinical_features):\n",
    "    sns.boxplot(data=df, x='target', y=feature, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of key clinical measurements:**\n",
    "-   **Resting Blood Pressure (`trestbps`)**: The median resting blood pressure is **almost the same as patients without disease**, though there are outliers with very high BP in both groups. Overall, resting BP does not appear to differ strongly by disease status in this sample.\n",
    "\n",
    "-   **Cholesterol (`chol`)**: CBoth groups show substantial overlap (and some extreme outliers ≥ 500 mg/dl), with only a small difference in the median levels. This suggests cholesterol alone may not be a highly discriminative feature in this particular dataset.\n",
    "\n",
    "-   **Maximum Heart Rate (`thalach`)**: The “Disease” group shows a higher median and overall distribution for maximum achieved heart rate compared to the no‐disease group. While it may seem counterintuitive (as one might expect limited heart‐rate response in more diseased hearts), in this dataset higher max‐HR measurements appear associated with having heart disease—likely reflecting the presence of individuals who can mount a stress response but still manifest other disease indicators.\n",
    "\n",
    "-   **ST Depression (`oldpeak`)**: Those **without disease** have a **higher median** ST depression, whereas patients with disease often show lower ST depression---though the \"no disease\" group has considerable variability and a number of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Medical Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key risk factors\n",
    "risk_factors = ['cp','fbs', 'exang', 'slope', 'ca', 'thal']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, factor in enumerate(risk_factors):\n",
    "    sns.countplot(data=df, x=factor, hue='target', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{factor} Distribution by Target')\n",
    "    axes[idx].legend(['No Disease', 'Disease'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical risk factors show distinct patterns:**\n",
    "- **Chest Pain Type (`cp`)**: **Typical angina (CP=0)** appears to be most prevalent among the no‐disease group, while atypical angina (CP=1), non‐anginal pain (CP=2), and asymptomatic presentations (CP=3) tend to be more common in the group that does have heart disease with **non-anginal pain** being most common in those with disease.\n",
    "\n",
    "- **Fasting Blood Sugar (`fbs`)**: The vast majority of both groups have a **normal fasting blood sugar (fbs=0).** A smaller share of each group shows elevated blood sugar (fbs=1).\n",
    "\n",
    "-   **Exercise-Induced Angina (`exang`)**: Having *no* exercise-induced angina (exang=0) is **more frequent** among those **with heart disease**; by contrast, exang=1 appears more in the no‐disease group.\n",
    "\n",
    "-   **Slope of ST Segment (`slope`)**: A **downsloping ST segment (slope=0)** is more associated with those who **do not have heart disease**, whereas an **upsloping ST segment (slope=2)** is much more common among disease‐positive individuals. **Flat ST slope (slope=2)** is present much more often in the no disease group vs those with disease but has a higher proportion of those with disease compared to slope 0.\n",
    "\n",
    "-   **Number of Major Vessels (`ca`)**: A large fraction of those **with disease** have **0 major vessels** colored by fluoroscopy, while those with **1 or more** vessels colored tend to be more in the no‐disease group.\n",
    "\n",
    "-   **Thalassemia Status (`thal`)**: \"Reversible defect\" (thal=3) occurs more often in the no disease group, along with \"normal\" (thal=1). \"Fixed defect\" (thal=2) occurs often in people with disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature correlations with target\n",
    "correlations = df.corr()['target'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlations.plot(kind='bar')\n",
    "plt.title('Feature Correlations with Target')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation Analysis:**\n",
    "\n",
    "#### **Key Correlations with the target (Heart Disease):**\n",
    "1. **Chest Pain Type (cp)** → *0.43*\n",
    "   - Moderate positive correlation\n",
    "   - Certain chest pain types indicate higher likelihood of heart disease\n",
    "   - Significant diagnostic indicator\n",
    "\n",
    "2. **Maximum Heart Rate (thalach)** → *0.42*\n",
    "   - Moderate positive correlation\n",
    "   - Higher exercise heart rates linked to increased disease probability\n",
    "\n",
    "3. **ST Segment Slope (slope)** → *0.35*\n",
    "   - Moderate positive correlation\n",
    "   - Steeper slopes during peak exercise associated with heart disease\n",
    "\n",
    "4. **Exercise-Induced Angina (exang)** → *-0.44*\n",
    "   - Moderate negative correlation\n",
    "   - Counterintuitive finding: exercise-induced chest pain linked to lower disease probability\n",
    "   - May indicate data imbalance or specific dataset patterns\n",
    "\n",
    "5. **ST Depression (oldpeak)** → *-0.44*\n",
    "   - Moderate negative correlation\n",
    "   - Unexpected finding: greater ST depression linked to lower disease risk\n",
    "   - Requires further clinical investigation\n",
    "\n",
    "6. **Number of Major Vessels (ca)** → *-0.38*\n",
    "   - Moderate negative correlation\n",
    "   - Unusual finding: more blocked vessels associated with lower risk\n",
    "   - Possible feature misinterpretation\n",
    "\n",
    "7. **Thalassemia (thal)** → *-0.34*\n",
    "   - Moderate negative correlation\n",
    "\n",
    "8. **Sex** → *-0.28*\n",
    "   - Males show slightly lower disease probability\n",
    "\n",
    "9. **Age** → *-0.23*\n",
    "   - Unexpected negative correlation with age\n",
    "   - Warrants further investigation\n",
    "\n",
    "#### **Notable Feature-to-Feature Correlations:**\n",
    "\n",
    "1. **oldpeak → slope**: *-0.58*\n",
    "  - Strong negative correlation between ST depression and slope\n",
    "\n",
    "2. **cp → exang**: *-0.40*\n",
    "  - Inverse relationship between chest pain types and exercise-induced angina\n",
    "\n",
    "3. **exang → thalach**: *-0.38*\n",
    "  - Exercise-induced angina associated with lower max heart rates\n",
    "\n",
    "4. **cp → thalach**: *0.31*\n",
    "  - Certain chest pain types correlate with higher exercise heart rates\n",
    "\n",
    "#### **Summary of Strongest Correlations**\n",
    "\n",
    "##### Positive Correlations with Heart Disease\n",
    "- Chest pain type (cp)\n",
    "- Maximum heart rate (thalach)\n",
    "- ST Segment Slope (slope)\n",
    "\n",
    "##### Negative Correlations with Heart Disease\n",
    "- ST depression (oldpeak)\n",
    "- Exercise induced angina (exang)\n",
    "- Number of vessels (ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "\n",
    "# Function to remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "    return df_clean\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "categorical_features = ['cp', 'thal', 'slope', 'restecg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and feature engineering\n",
    "\n",
    "# Handle outliers in numerical features\n",
    "df_clean = remove_outliers(df, numerical_features)\n",
    "print(f\"Rows after outlier removal: {len(df_clean)} (removed {len(df) - len(df_clean)} rows)\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop('target', axis=1)\n",
    "y = df_clean['target']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split and scaling\n",
    "\n",
    "# Split the data\n",
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale only the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = X_train_unscaled.copy()\n",
    "X_test = X_test_unscaled.copy()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train_unscaled[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test_unscaled[numerical_features])\n",
    "\n",
    "print(\"\\nTraining set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "print(\"\\nFeatures after one-hot encoding:\", X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Feature_selection\n",
    "importlib.reload(Feature_selection)\n",
    "from Feature_selection import autoFeatureSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `num_output_features` specifies the number of features to select for each method.\n",
    "# The `autoFeatureSelector` function applies multiple feature selection methods \n",
    "# (e.g., Pearson correlation, chi-square test, RFE, etc.) and combines their results\n",
    "# by voting to identify the most important features.\n",
    "\n",
    "best_features = autoFeatureSelector(\n",
    "    X,\n",
    "    y,\n",
    "    methods=['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm'],  # List of feature selection methods\n",
    "    num_output_features=5  # Number of features to select per method\n",
    ")\n",
    "\n",
    "# Output the final list of selected features after voting\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs = X[['oldpeak', 'thalach', 'cp', 'ca', 'thal']]\n",
    "X_fs = pd.get_dummies(X_fs, columns=['thal', 'cp'])\n",
    "X_fs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both models\n",
    "sgd_default = SGDClassifier(random_state=42)\n",
    "sgd_tuned = SGDClassifier(\n",
    "    random_state=42,\n",
    "    loss='modified_huber',  # Better handling of probabilities\n",
    "    class_weight='balanced', # Handle class imbalance\n",
    "    max_iter=1000,          # Ensure convergence\n",
    "    tol=1e-3               # Convergence criterion\n",
    ")\n",
    "\n",
    "# Train both models\n",
    "sgd_default.fit(X_train, y_train)\n",
    "sgd_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for both models\n",
    "y_pred_default = sgd_default.predict(X_test)\n",
    "y_pred_tuned = sgd_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "print(\"Default SGD Classifier Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_default))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_default))\n",
    "\n",
    "print(\"\\nTuned SGD Classifier Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# Compare confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot confusion matrix for default model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_default), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1)\n",
    "ax1.set_title(\"Default SGD Confusion Matrix\")\n",
    "ax1.set_xlabel(\"Predicted\")\n",
    "ax1.set_ylabel(\"Actual\")\n",
    "\n",
    "# Plot confusion matrix for tuned model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_tuned), \n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax2)\n",
    "ax2.set_title(\"Tuned SGD Confusion Matrix\")\n",
    "ax2.set_xlabel(\"Predicted\")\n",
    "ax2.set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and compare class probabilities (only available for tuned model)\n",
    "if hasattr(sgd_tuned, \"predict_proba\"):\n",
    "    probabilities = sgd_tuned.predict_proba(X_test)\n",
    "    print(\"\\nProbability distribution for first 5 predictions:\")\n",
    "    print(\"Class 0 (No Disease) | Class 1 (Disease)\")\n",
    "    print(probabilities[:5])\n",
    "else:\n",
    "    print(\"\\nDefault SGD model doesn't support probability estimates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation analysis\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(sgd_model, X_train, y_train, cv=5)\n",
    "print(\"\\nCross-validation scores:\", cv_scores)\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    sgd_model, X_train, y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, n_jobs=-1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')\n",
    "plt.plot(train_sizes, val_scores.mean(axis=1), label='Cross-validation score')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves for SGD Classifier')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'loss': ['hinge', 'log_loss', 'modified_huber'],  # Different loss functions\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],           # Regularization methods\n",
    "    'alpha': [0.0001, 0.001, 0.01],                 # Regularization strength\n",
    "    'learning_rate': ['constant', 'optimal', 'adaptive'],  # Learning rate schedule\n",
    "    'eta0': [0.01, 0.1, 1],                         # Initial learning rate\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(SGDClassifier(random_state=42, max_iter=1000, tol=1e-3),\n",
    "                           param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest Parameters Found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "print(\"\\nTuned SGD Classifier Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the tuned model\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_tuned), annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False)\n",
    "plt.title(\"Confusion Matrix (Tuned Model)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Removal and Re-Evaluation\n",
    "#Randomly or strategically remove features to observe how it impacts model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a subset of features (e.g., drop the 'chol' and 'thalach' columns)\n",
    "X_train_reduced = X_train.drop(['chol', 'thalach'], axis=1)\n",
    "X_test_reduced = X_test.drop(['chol', 'thalach'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the SGD model with reduced features\n",
    "sgd_model_reduced = SGDClassifier(random_state=42)\n",
    "sgd_model_reduced.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the reduced model\n",
    "y_pred_reduced = sgd_model_reduced.predict(X_test_reduced)\n",
    "print(\"\\nSGD Classifier Results (Reduced Features):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_reduced))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the reduced model\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_reduced), annot=True, fmt=\"d\", cmap=\"Oranges\", cbar=False)\n",
    "plt.title(\"Confusion Matrix (Reduced Features)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model (No Feature Selection, No Hyperparameter Tuning)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_base = RandomForestClassifier()\n",
    "rf_model_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_base = rf_model_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_base = accuracy_score(y_test, y_pred_base)\n",
    "print(f'Accuracy: {accuracy_base}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline with Hyperparamaters Tuning (No Feature Selection, with GridSearchCV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  \n",
    "    'max_depth': [2, 3, 5],  \n",
    "    'min_samples_split': [2, 10, 20],  \n",
    "    'min_samples_leaf': [1, 5, 10],  \n",
    "    'max_features': ['sqrt', 'log2']  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_base_tuned = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid,\n",
    "    cv = 5, \n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "rf_model_base_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_base_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_base_tuned = rf_model_base_tuned.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_base_tuned = accuracy_score(y_test, y_pred_base_tuned)\n",
    "print(f'Accuracy: {accuracy_base_tuned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature-Selected Model (With Feature Selection, No GridSearchCV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_fs = RandomForestClassifier()\n",
    "rf_model_fs.fit(X_train_fs, y_train_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fs = rf_model_fs.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_fs = accuracy_score(y_test_fs, y_pred_fs)\n",
    "print(f'Accuracy: {accuracy_fs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature-Selected with Hyperparameter Tuning (With Feature Selection, With GridSearchCV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_fs_tuned = GridSearchCV(\n",
    "    RandomForestClassifier(), \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs = -1, \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "rf_model_fs_tuned.fit(X_train_fs, y_train_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_fs_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fs_tuned = rf_model_fs_tuned.best_estimator_.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_fs_tuned = accuracy_score(y_test_fs, y_pred_fs_tuned)\n",
    "print(f'Accuracy: {accuracy_fs_tuned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for 4 different models\n",
    "y_pred_1 = rf_model_base.predict(X_test)         # Model 1: Clean data, no feature selection\n",
    "y_pred_2 = rf_model_base_tuned.best_estimator_.predict(X_test)   # Model 2: Clean data, no feature selection, with GridSearchCV\n",
    "y_pred_3 = rf_model_fs.predict(X_test_fs)           # Model 3: Clean data, with feature selection\n",
    "y_pred_4 = rf_model_fs_tuned.best_estimator_.predict(X_test_fs)    # Model 4: Clean data, feature selection, with GridSearchCV\n",
    "\n",
    "# Compute confusion matrices\n",
    "cm_1 = confusion_matrix(y_test, y_pred_base)\n",
    "cm_2 = confusion_matrix(y_test, y_pred_base_tuned)\n",
    "cm_3 = confusion_matrix(y_test, y_pred_fs)\n",
    "cm_4 = confusion_matrix(y_test, y_pred_fs_tuned)\n",
    "\n",
    "# Create subplots (2 rows, 2 columns)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Define model names for better visualization\n",
    "titles = [\n",
    "    \"Model 1: Baseline (No FS, No GridSearch)\",\n",
    "    \"Model 2: Baseline (No FS, With GridSearch)\",\n",
    "    \"Model 3: With Feature Selection (No GridSearch)\",\n",
    "    \"Model 4: With Feature Selection (With GridSearch)\"\n",
    "]\n",
    "\n",
    "# Function to plot confusion matrix using seaborn heatmap\n",
    "def plot_confusion_matrix(ax, cm, title):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=[\"No Disease\", \"Disease\"], yticklabels=[\"No Disease\", \"Disease\"], ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "\n",
    "# Plot all four confusion matrices\n",
    "for ax, cm, title in zip(axes.ravel(), [cm_1, cm_2, cm_3, cm_4], titles):\n",
    "    plot_confusion_matrix(ax, cm, title)\n",
    "\n",
    "# Adjust layout for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion on Model Performance and Overfitting**\n",
    "\n",
    "1. **Overfitting in the Baseline Models**\n",
    "   - **Before Feature Selection (Model 1 & Model 2)**\n",
    "     - Model 1 (Baseline, No FS, No GridSearch) achieves perfect classification:\n",
    "       - True Positives (TP): 146\n",
    "       - True Negatives (TN): 144\n",
    "       - No misclassifications (0 FP, 0 FN).\n",
    "       - This suggests the model might be overfitting, as perfect accuracy on a real-world dataset is uncommon.\n",
    "     - Model 2 (Baseline, No FS, With GridSearch) introduces misclassifications:\n",
    "       - FP increases to 16, FN increases to 11.\n",
    "       - A slight drop in accuracy, indicating that GridSearchCV might be tuning hyperparameters in a way that reduces overfitting.\n",
    "\n",
    "2. **Feature Selection Impacts Generalization**\n",
    "   - **After Feature Selection (Model 3 & Model 4)**\n",
    "     - Model 3 (With Feature Selection, No GridSearch):\n",
    "       - FP: 9, FN: 22.\n",
    "       - More misclassifications compared to Model 1.\n",
    "       - While accuracy decreases slightly, this indicates better generalization, as the model is no longer memorizing the dataset.\n",
    "     - Model 4 (With Feature Selection, With GridSearch):\n",
    "       - FP increases to 30, FN decreases to 20.\n",
    "       - This suggests that hyperparameter tuning, specifically adjusting **max_depth**, helps control overfitting and improves the model's ability to perform well on unseen data.\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **Baseline Model 1 exhibits possible overfitting** (perfect accuracy).\n",
    "- **Feature selection improves generalization but introduces more misclassifications** (Models 3 & 4).\n",
    "- **GridSearchCV helps prevent overfitting** by tuning parameters like **max_depth**, leading to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_base = SVC(random_state=42)\n",
    "svc_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_svc_base = svc_base.predict(X_train)\n",
    "y_pred_svc_base = svc_base.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_svc_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy_svc_base = accuracy_score(y_train, y_pred_train_svc_base)\n",
    "test_accuracy_svc_base = accuracy_score(y_test, y_pred_svc_base)\n",
    "print(\"SVC baseline training dataset accuracy score\", training_accuracy_svc_base)\n",
    "print(\"SVC baseline testing dataset accuracy score\", test_accuracy_svc_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Base Model Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000], \n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "    'kernel': ['rbf']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid = GridSearchCV(SVC(), svc_param_grid, refit=True, verbose=3)  # refits the estimator using the best found parameters\n",
    "svc_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid_predictions = svc_grid.predict(X_test)\n",
    "confusion_matrix(y_test, svc_grid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, svc_grid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Feature-Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_fs = SVC(random_state=42)\n",
    "svc_fs.fit(X_train_fs, y_train_fs)\n",
    "y_predict_train_svc_fs = svc_fs.predict(X_train_fs)\n",
    "y_predict_test_svc_fs = svc_fs.predict(X_test_fs)\n",
    "accuracy_train_svc_fs = accuracy_score(y_train_fs, y_predict_train_svc_fs)\n",
    "accuracy_test_svc_fs = accuracy_score(y_test_fs, y_predict_test_svc_fs)\n",
    "print(\"SVC feature selected train dataset accuracy score:\", accuracy_train_svc_fs)\n",
    "print(\"SVC feature selected test dataset accuracy score:\", accuracy_test_svc_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Feature-Selected Hyperparameter tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_fs_grid = GridSearchCV(SVC(), svc_param_grid, refit=True, verbose=3)  # refits the estimator using the best found parameters\n",
    "svc_fs_grid.fit(X_train_fs, y_train_fs)\n",
    "y_predict_train_svc_fs_grid = svc_fs_grid.predict(X_train_fs)\n",
    "y_predict_test_svc_fs_grid = svc_fs_grid.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_svc_fs_grid = accuracy_score(y_train_fs, y_predict_train_svc_fs_grid)\n",
    "accuracy_test_svc_fs_grid = accuracy_score(y_test_fs, y_predict_test_svc_fs_grid)\n",
    "print(\"SVC feature selected TUNED train dataset accuracy score:\", accuracy_train_svc_fs_grid)\n",
    "print(\"SVC feature selected TUNED test dataset accuracy score:\", accuracy_test_svc_fs_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_fs, y_predict_test_svc_fs_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
